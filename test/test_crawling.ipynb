{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: konlpy in c:\\programdata\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.25.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: transformers in c:\\users\\hyerim\\appdata\\roaming\\python\\python38\\site-packages (4.46.3)\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex in c:\\users\\hyerim\\appdata\\roaming\\python\\python38\\site-packages (from nltk) (2021.9.30)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.4.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (4.6.3)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.20.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hyerim\\appdata\\roaming\\python\\python38\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.3.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\hyerim\\appdata\\roaming\\python\\python38\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\hyerim\\appdata\\roaming\\python\\python38\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.5.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\hyerim\\appdata\\roaming\\python\\python38\\site-packages (from torch) (4.12.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -sspec (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yodbc (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -spec (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Error parsing dependencies of pyodbc: Invalid version: '4.0.0-unsupported'\n",
      "WARNING: Error parsing dependencies of pyzmq: Invalid version: 'cpython'\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -sspec (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yodbc (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -spec (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hyerim\\appdata\\roaming\\python\\python38\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk konlpy beautifulsoup4 requests pandas transformers torch --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일이 성공적으로 저장되었습니다: kospi_stocks.csv, kosdaq_stocks.csv\n",
      "0    000250\n",
      "1    000440\n",
      "2    001000\n",
      "3    001540\n",
      "4    001810\n",
      "5    001840\n",
      "6    002230\n",
      "7    002290\n",
      "8    002680\n",
      "9    002800\n",
      "Name: 단축코드, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 코스피, 코스닥 종목명 및 코드 추출\n",
    "\n",
    "def extract_stocks_name():\n",
    "    df_kospi = pd.read_excel('kospi_code.xlsx', dtype={'단축코드': str})\n",
    "    df_kosdaq = pd.read_excel('kosdaq_code.xlsx', dtype={'단축코드': str})\n",
    "\n",
    "    filterd_df_kospi = df_kospi[df_kospi['그룹코드'] == 'ST']\n",
    "    filterd_df_kosdaq = df_kosdaq[df_kosdaq['증권그룹구분코드'] == 'ST']\n",
    "\n",
    "    result_df_kospi = filterd_df_kospi[['한글명', '단축코드']].copy()\n",
    "    result_df_kosdaq = filterd_df_kosdaq[['한글종목명', '단축코드']].copy()\n",
    "    \n",
    "    \n",
    "    result_df_kospi.rename(columns={'한글명': '종목명'}, inplace=True)\n",
    "    result_df_kosdaq.rename(columns={'한글종목명': '종목명'}, inplace=True)\n",
    "\n",
    "    result_df_kospi['단축코드'] = result_df_kospi['단축코드'].apply(lambda x: x.zfill(6))\n",
    "    result_df_kosdaq['단축코드'] = result_df_kosdaq['단축코드'].apply(lambda x: x.zfill(6))\n",
    "\n",
    "    result_df_kospi.to_csv('../data/raw_data/kospi_stocks.csv', index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)\n",
    "    result_df_kosdaq.to_csv('../data/raw_data/kosdaq_stocks.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"CSV 파일이 성공적으로 저장되었습니다: kospi_stocks.csv, kosdaq_stocks.csv\")\n",
    "\n",
    "    #df = pd.read_csv('kosdaq_stocks.csv')\n",
    "    #print(df['단축코드'].head(10))\n",
    "    \n",
    "extract_stocks_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_previous_posts(base_url):\n",
    "    posts = []\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    today = datetime.now()\n",
    "    date_start = (today - timedelta(days=1)).replace(hour=0, minute=0, second=0)\n",
    "    # date_end = (today - timedelta(days=1)).replace(hour=23, minute=59, second=59)\n",
    "    \n",
    "    page = 0\n",
    "    while True:\n",
    "        url = f\"{base_url}?&od=T31&category=0&po={page}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch page {page}: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = soup.select('div.list_item.symph_row')\n",
    "        \n",
    "        if not articles:\n",
    "            print(\"No more articles found.\")\n",
    "            break\n",
    "        \n",
    "        for article in articles:\n",
    "            # Extract title and link\n",
    "            title_element = article.select_one('span.subject_fixed')\n",
    "            date_element = article.select_one('span.timestamp')\n",
    "            link_element = article.select_one('a.list_subject')\n",
    "            \n",
    "            if not title_element or not date_element or not link_element:\n",
    "                continue\n",
    "            \n",
    "            title = title_element.get_text(strip=True)\n",
    "            date_str = date_element.get_text(strip=True)\n",
    "            link = f\"https://www.clien.net{link_element['href']}\"\n",
    "            \n",
    "            # Convert date to datetime object\n",
    "            post_date = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            # Stop if the post date is before range\n",
    "            if post_date < date_start:\n",
    "                print(\"No more posts from yesterday.\")\n",
    "                return posts\n",
    "\n",
    "            if not (date_start <= post_date <= today):\n",
    "                continue\n",
    "            \n",
    "            # Crawling post content\n",
    "            post_response = requests.get(link, headers=headers)\n",
    "            if post_response.status_code != 200:\n",
    "                print(f\"Failed to fetch post: {link}\")\n",
    "                continue\n",
    "            \n",
    "            post_soup = BeautifulSoup(post_response.text, 'html.parser')\n",
    "            content_element = post_soup.select_one('div.post_article')\n",
    "            content = content_element.get_text(strip=True) if content_element else \"No content\"\n",
    "            \n",
    "            posts.append({\n",
    "                'title': title,\n",
    "                'date': post_date.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "                'text': content,\n",
    "                'link' : link\n",
    "            })\n",
    "        page += 1\n",
    "\n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stock_names():\n",
    "    df_kospi = pd.read_csv('../data/raw_data/kospi_stocks.csv')  # KOSPI 종목명 파일\n",
    "    df_kosdaq = pd.read_csv('../data/raw_data/kosdaq_stocks.csv')  # KOSDAQ 종목명 파일\n",
    "    \n",
    "    # 종목명 컬럼 병합\n",
    "    stock_names = pd.concat([df_kospi['종목명'], df_kosdaq['종목명']])\n",
    "    stock_names = stock_names.dropna().str.strip().tolist()\n",
    "    \n",
    "    # 긴 이름이 우선 매칭되도록 정렬\n",
    "    stock_names.sort(key=len, reverse=True)\n",
    "    \n",
    "    return stock_names\n",
    "\n",
    "def extract_stock_keywords(posts):\n",
    "    # 주식 종목 명 우선 추출\n",
    "    stock_names = load_stock_names()\n",
    "    \n",
    "    # Load KoBERT model, tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "    model = BertModel.from_pretrained('monologg/kobert')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for post in posts:\n",
    "        text = post['title'] + \" \" + post['text']\n",
    "        text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', text) # 특수 문자 제거 및 전처리\n",
    "        \n",
    "        # 종목명 우선 매칭\n",
    "        match_stock_names = []\n",
    "        for stock in stock_names:\n",
    "            if stock in text:\n",
    "                match_stock_names.append(stock)\n",
    "                text = text.replace(stock, '')  # 이미 매칭된 종목명은 제거\n",
    "                \n",
    "        # KoBERT로 남은 텍스트 토큰화 (KoBERT 최대 길이 = 512)\n",
    "        split_texts = [text[i:i+512] for i in range(0, len(text), 512)] \n",
    "        keywords = []\n",
    "        \n",
    "        for chunk in split_texts:\n",
    "            # 토큰화\n",
    "            inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n",
    "            \n",
    "            # KoBERT 모델로 인코딩\n",
    "            # gradient 계산 비활성화, 훈련 과정이 아니므로 시간 단축을 위해서\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            # 토큰 ID 배열(['input_ids']) -> 텍스트 변환\n",
    "            input_ids = inputs['input_ids'][0]\n",
    "            tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "            \n",
    "            # 키워드 필터링: 길이가 1보다 길고, 한국어 문자만 포함\n",
    "            keywords.extend([token for token in tokens if len(token) > 1 and re.match(r'[가-힣]+', token)])\n",
    "        \n",
    "        \n",
    "        keyword_counts_dict = dict(Counter(match_stock_names + keywords))\n",
    "        \n",
    "        results.append({\n",
    "            'title': post['title'],\n",
    "            'date': post['date'],\n",
    "            'text': post['text'],\n",
    "            'link': post['link'],\n",
    "            'keywords': keyword_counts_dict\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more posts from yesterday.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Raw data schema]\n",
      "\n",
      "                                     title              date  \\\n",
      "0                    오늘 시간외 특징주 요약 - 1월 6일  2025-01-06 19:01   \n",
      "1                    오늘 장마감 특징주 요약 - 1월 6일  2025-01-06 17:54   \n",
      "2           오늘 장마감 주도주 / 테마주 순위 요약 - 1월 6일  2025-01-06 15:37   \n",
      "3                 주식 소각한 다음부터 주가는 상승이 많나요?  2025-01-06 15:25   \n",
      "4  2024년 연금저축, ISA, IRP 투자 따라하기 (feat 물타님)  2025-01-06 15:14   \n",
      "\n",
      "                                                text  \\\n",
      "0  오늘 시간외 특징주 요약 - 1월 6일텍스트 버전은 하단에 ..**출처 : 테마랩오...   \n",
      "1  오늘 장마감 특징주 요약 - 1월 6일텍스트 버전은 하단에 ..**출처 : 테마랩정...   \n",
      "2  오늘 장마감 주도주 / 테마주 순위 요약 - 1월 6일텍스트 버전은 이미지 하단에 ...   \n",
      "3  오늘 한 기업이 주식을 소각하는 날 입니다.그러면 향후 주가는 상승할 확률이 높나요...   \n",
      "4  클리앙에 물타님의 연금저축, ISA, IRP 투자를 보고 따라한 후기 입니다.연금성...   \n",
      "\n",
      "                                                link  \\\n",
      "0  https://www.clien.net/service/board/cm_stock/1...   \n",
      "1  https://www.clien.net/service/board/cm_stock/1...   \n",
      "2  https://www.clien.net/service/board/cm_stock/1...   \n",
      "3  https://www.clien.net/service/board/cm_stock/1...   \n",
      "4  https://www.clien.net/service/board/cm_stock/1...   \n",
      "\n",
      "                                            keywords  \n",
      "0  {'HB테크놀러지': 1, '에프엔에스테크': 1, '에스와이스틸텍': 1, '전진...  \n",
      "1  {'HD현대에너지솔루션': 1, '켄코아에어로스페이스': 1, '바이오로그디바이스'...  \n",
      "2  {'코오롱모빌리티그룹우': 1, '코오롱모빌리티그룹': 1, '바이오로그디바이스':...  \n",
      "3                                 {'주식': 1, '오늘': 1}  \n",
      "4  {'DB': 1, '투자': 5, '보고': 1, '좋은': 1, '금액': 1, ...  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://www.clien.net/service/board/cm_stock\"\n",
    "    posts = get_previous_posts(base_url)\n",
    "    \n",
    "    if posts:\n",
    "        raw_data_df = extract_stock_keywords(posts)\n",
    "        print(\"[Raw data schema]\\n\")\n",
    "        print(raw_data_df.head())\n",
    "    else:\n",
    "        print(\"No keywords found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
