{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4 requests pandas transformers torch --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코스피, 코스닥 종목명 및 코드 추출\n",
    "def extract_stocks_name():\n",
    "    df_kospi = pd.read_excel('kospi_code.xlsx', dtype={'단축코드': str})\n",
    "    df_kosdaq = pd.read_excel('kosdaq_code.xlsx', dtype={'단축코드': str})\n",
    "\n",
    "    filterd_df_kospi = df_kospi[df_kospi['그룹코드'] == 'ST']\n",
    "    filterd_df_kosdaq = df_kosdaq[df_kosdaq['증권그룹구분코드'] == 'ST']\n",
    "\n",
    "    result_df_kospi = filterd_df_kospi[['한글명', '단축코드']].copy()\n",
    "    result_df_kosdaq = filterd_df_kosdaq[['한글종목명', '단축코드']].copy()\n",
    "    \n",
    "    \n",
    "    result_df_kospi.rename(columns={'한글명': '종목명'}, inplace=True)\n",
    "    result_df_kosdaq.rename(columns={'한글종목명': '종목명'}, inplace=True)\n",
    "\n",
    "    result_df_kospi['단축코드'] = result_df_kospi['단축코드'].apply(lambda x: x.zfill(6))\n",
    "    result_df_kosdaq['단축코드'] = result_df_kosdaq['단축코드'].apply(lambda x: x.zfill(6))\n",
    "\n",
    "    result_df_kospi.to_csv('../data/kospi_stocks.csv', index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)\n",
    "    result_df_kosdaq.to_csv('../data/kosdaq_stocks.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"CSV 파일이 성공적으로 저장되었습니다: kospi_stocks.csv, kosdaq_stocks.csv\")\n",
    "\n",
    "    #df = pd.read_csv('kosdaq_stocks.csv')\n",
    "    #print(df['단축코드'].head(10))\n",
    "    \n",
    "extract_stocks_name()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clien_posts(base_url):\n",
    "    posts = []\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    today = datetime.now()\n",
    "    date_start = (today - timedelta(days=1)).replace(hour=0, minute=0, second=0)\n",
    "    date_end = (today - timedelta(days=1)).replace(hour=23, minute=59, second=59)\n",
    "    \n",
    "    page = 0\n",
    "    while True:\n",
    "        url = f\"{base_url}?&od=T31&category=0&po={page}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch page {page}: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = soup.select('div.list_item.symph_row')\n",
    "        \n",
    "        if not articles:\n",
    "            print(\"[Clien] No more articles found.\")\n",
    "            break\n",
    "        \n",
    "        for article in articles:\n",
    "            # Extract title and link\n",
    "            title_element = article.select_one('span.subject_fixed')\n",
    "            date_element = article.select_one('span.timestamp')\n",
    "            link_element = article.select_one('a.list_subject')\n",
    "            \n",
    "            if not title_element or not date_element or not link_element:\n",
    "                continue\n",
    "            \n",
    "            title = title_element.get_text(strip=True)\n",
    "            date_str = date_element.get_text(strip=True)\n",
    "            link = f\"https://www.clien.net{link_element['href']}\"\n",
    "            \n",
    "            # Convert date to datetime object\n",
    "            post_date = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            # Stop if the post date is before range\n",
    "            if post_date < date_start:\n",
    "                print(\"[Clien] No more posts from yesterday.\")\n",
    "                return posts\n",
    "\n",
    "            if not (date_start <= post_date <= date_end):\n",
    "                continue\n",
    "            \n",
    "            # Crawling post content\n",
    "            post_response = requests.get(link, headers=headers)\n",
    "            if post_response.status_code != 200:\n",
    "                print(f\"[Clien] Failed to fetch post: {link}\")\n",
    "                continue\n",
    "            \n",
    "            post_soup = BeautifulSoup(post_response.text, 'html.parser')\n",
    "            content_element = post_soup.select_one('div.post_article')\n",
    "            content = content_element.get_text(strip=True) if content_element else \"No content\"\n",
    "            \n",
    "            posts.append({\n",
    "                'title': title,\n",
    "                'date': post_date.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "                'text': content,\n",
    "                'link' : link\n",
    "            })\n",
    "        page += 1\n",
    "\n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fmkorea_posts(base_url):\n",
    "    posts = []\n",
    "    \n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    today = datetime.now()\n",
    "    yesterday = today - timedelta(days=1)\n",
    "    date_start = (today - timedelta(days=1)).replace(hour=0, minute=0, second=0)\n",
    "    date_end = (today - timedelta(days=1)).replace(hour=23, minute=59, second=59)\n",
    "    \n",
    "    page = 1\n",
    "    while True:\n",
    "        url = f\"{base_url}&page={page}\"\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(60) \n",
    "        \n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, '.bd_lst.bd_tb_lst.bd_tb .title a')\n",
    "        stop_crawling = False\n",
    "        \n",
    "        if not articles:\n",
    "            print(\"[FMKorea] No more articles found.\")\n",
    "            break\n",
    "        \n",
    "        for article in articles:\n",
    "            if article.get_attribute('title') == \"댓글\":\n",
    "                continue\n",
    "                \n",
    "            parent_tr = article.find_element(By.XPATH, \"./../..\")  # article의 부모 tr 태그를 찾기\n",
    "            date_element = parent_tr.find_element(By.CSS_SELECTOR, \".time\")\n",
    "            date_str = date_element.text.strip()\n",
    "                \n",
    "            try:\n",
    "                if \":\" in date_str: # HH:MM 형태 시간 계산\n",
    "                    post_time = datetime.strptime(date_str, \"%H:%M\").time()\n",
    "                    post_date = datetime.combine(today.date(), post_time)\n",
    "                    if post_date > today:  # HH:MM이 오늘 이후라면 어제로 조정\n",
    "                        post_date = datetime.combine(yesterday.date(), post_time)\n",
    "                else:\n",
    "                    post_date = datetime.strptime(date_str, \"%Y.%m.%d %H:%M\")\n",
    "            except Exception as e:\n",
    "                print(f\"[FMKorea] Date parsing failed for {date_str}: {e}\")\n",
    "                continue\n",
    "                \n",
    "            # 첫번째 어제자 게시물을 찾으면, 본문 크롤링만 진행\n",
    "            if date_start <= post_date <= date_end:\n",
    "                print(\"[FMKorea] Found first yesterday post, starting detailed crawls...\")\n",
    "                article.click()\n",
    "                time.sleep(2)\n",
    "                \n",
    "                while True:\n",
    "                    title = driver.find_element(By.CSS_SELECTOR, 'h1.np_18px').text.strip()\n",
    "                    url = driver.current_url\n",
    "                    time_element = driver.find_element(By.CSS_SELECTOR, '.date')\n",
    "                    time_str = time_element.text.strip()\n",
    "                    date = datetime.strptime(time_str, \"%Y.%m.%d %H:%M\")\n",
    "                    \n",
    "                    if date < date_start:\n",
    "                        stop_crawling = True\n",
    "                        break\n",
    "                    \n",
    "                    content_element = driver.find_element(By.CSS_SELECTOR, '.rd_body article')\n",
    "                    content = content_element.text.strip() if content_element else \"No content\"\n",
    "                    \n",
    "                    posts.append({\n",
    "                        'title': title,\n",
    "                        'date': date.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "                        'text': content,\n",
    "                        'link': url\n",
    "                    })\n",
    "                    \n",
    "                    # 다음 게시글로 넘어가도록 버튼 클릭\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, 'div.prev_next_btns span.btn_pack.next.blockfmcopy')\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "                    next_button.click()\n",
    "                    time.sleep(0.5)\n",
    "                \n",
    "                if stop_crawling:\n",
    "                    break\n",
    "                \n",
    "        if stop_crawling:\n",
    "            print(\"[FMKorea] Crawling stopped. All posts from yesterday have been fetched.\")\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "    \n",
    "    driver.quit()\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopword.txt \n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = set(line.strip() for line in f if line.strip())\n",
    "    return stopwords\n",
    "\n",
    "# Load csv file \n",
    "def load_stock_names():\n",
    "    df_kospi = pd.read_csv('../data/raw_data/kospi_stocks.csv')  # KOSPI 종목명 파일\n",
    "    df_kosdaq = pd.read_csv('../data/raw_data/kosdaq_stocks.csv')  # KOSDAQ 종목명 파일\n",
    "    \n",
    "    # 종목명 컬럼 병합\n",
    "    stock_names = pd.concat([df_kospi['종목명'], df_kosdaq['종목명']])\n",
    "    stock_names = stock_names.dropna().str.strip().tolist()\n",
    "    stock_names.sort(key=len, reverse=True)\n",
    "    \n",
    "    return stock_names\n",
    "\n",
    "# Load alias json (대체어 저장 파일)\n",
    "def load_stock_aliases(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        alias_dict = json.load(f)\n",
    "    \n",
    "    return alias_dict\n",
    "\n",
    "# Match stock name with alias json \n",
    "def match_stock_names_with_aliases(text, stock_names, stock_aliases):\n",
    "    matched_stocks = []\n",
    "    alias_to_main = {}  # Alias → 대표 종목명 매핑\n",
    "\n",
    "    # Alias 사전 구성\n",
    "    for main_stock, aliases in stock_aliases.items():\n",
    "        alias_to_main[main_stock] = main_stock  # 대표 종목명도 매핑\n",
    "        for alias in aliases:\n",
    "            alias_to_main[alias] = main_stock   # Alias → 대표 종목명 매핑\n",
    "\n",
    "    extended_stock_names = set(alias_to_main.keys())\n",
    "\n",
    "    # 긴 이름부터 매칭\n",
    "    for stock in sorted(extended_stock_names, key=len, reverse=True):\n",
    "        if stock in text:\n",
    "            matched_stocks.append(alias_to_main[stock])  # 대표 종목명으로 저장\n",
    "            text = text.replace(stock, '')               # 매칭된 종목명 제거\n",
    "\n",
    "    return matched_stocks, text\n",
    "\n",
    "# Extract keywords to use KoBERT\n",
    "def extract_keywords_with_kobert(text, tokenizer, model):\n",
    "    # 불용어 리스트 생성\n",
    "    STOPWORDS_FILE = '../data/stopwords.txt'\n",
    "    STOPWORDS = load_stopwords(STOPWORDS_FILE)\n",
    "    \n",
    "    keywords = []\n",
    "    \n",
    "    split_texts = [text[i:i+512] for i in range(0, len(text), 512)]\n",
    "    \n",
    "    for chunk in split_texts:\n",
    "        # KoBERT 토큰화\n",
    "        inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # 토큰 ID 배열 -> 토큰으로 변환\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        \n",
    "        # 토큰 재결합 (서브워드 합치기)\n",
    "        combined_tokens = []\n",
    "        temp_token = ''\n",
    "        for token in tokens:\n",
    "            if token.startswith('##'):\n",
    "                temp_token += token[2:]  # ## 제거 후 이어붙이기\n",
    "            else:\n",
    "                if temp_token:\n",
    "                    combined_tokens.append(temp_token)\n",
    "                    temp_token = ''\n",
    "                if len(token) > 1 and re.match(r'[가-힣a-zA-Z]+', token):\n",
    "                    combined_tokens.append(token)\n",
    "        \n",
    "        # 마지막 남은 토큰 추가\n",
    "        if temp_token:\n",
    "            combined_tokens.append(temp_token)\n",
    "        \n",
    "        # STOPWORDS 필터링\n",
    "        keywords.extend([token for token in combined_tokens if token not in STOPWORDS])\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "# 메인 기능 함수\n",
    "def extract_stock_keywords(posts):\n",
    "    stock_names = load_stock_names()\n",
    "    stock_aliases = load_stock_aliases('../data/raw_data/stock_alias.json')\n",
    "    \n",
    "    # Load KoBERT model, tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "    model = BertModel.from_pretrained('monologg/kobert')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for post in posts:\n",
    "        text = str(post['title']) + \" \" + str(post['text'])\n",
    "        text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', text) # 특수 문자 제거 및 전처리\n",
    "        \n",
    "        # 1. 종목명 및 Alias 매칭\n",
    "        matched_stocks, text = match_stock_names_with_aliases(text, stock_names, stock_aliases)\n",
    "        # 2. KoBERT 키워드 추출 및 재결합\n",
    "        kobert_keywords = extract_keywords_with_kobert(text, tokenizer, model)\n",
    "        \n",
    "        keyword_counts = Counter(matched_stocks + kobert_keywords)\n",
    "        keyword_counts_dict = dict(sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)) # 내림차순 정렬\n",
    "        \n",
    "        results.append({\n",
    "            'title': post['title'],\n",
    "            'date': post['date'],\n",
    "            'text': post['text'],\n",
    "            'link': post['link'],\n",
    "            'keywords': keyword_counts_dict\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    combined_df = pd.read_csv(\"../data/raw_data/community_crawling_data.csv\")\n",
    "    posts = []\n",
    "    for _, row in combined_df.iterrows():\n",
    "        posts.append({\n",
    "            'title': row['title'],\n",
    "            'date': row['date'],\n",
    "            'text': row['text'],\n",
    "            'link': row['link'],\n",
    "        })\n",
    "    \n",
    "    raw_data_df = extract_stock_keywords(posts)\n",
    "    raw_data_df.to_csv('../data/raw_data/community_raw_data.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    clien_url = \"https://www.clien.net/service/board/cm_stock\"\n",
    "    fm_url = \"https://www.fmkorea.com/index.php?mid=stock&category=2997203870\"\n",
    "    \n",
    "    clien_posts = get_clien_posts(clien_url)\n",
    "    fm_posts = get_fmkorea_posts(fm_url)\n",
    "    \n",
    "    crawling_df = pd.DataFrame(clien_posts + fm_posts)\n",
    "    crawling_df.to_csv('../data/raw_data/community_crawling_data.csv', index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    clien_raw_data_df = extract_stock_keywords(clien_posts)\n",
    "    fm_raw_data_df = extract_stock_keywords(fm_posts)\n",
    "\n",
    "    combined_df = pd.concat([clien_raw_data_df, fm_raw_data_df], ignore_index=True)\n",
    "    combined_df.to_csv('../data/raw_data/community_raw_data.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
