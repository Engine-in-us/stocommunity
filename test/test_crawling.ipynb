{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk konlpy beautifulsoup4 requests pandas transformers torch --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코스피, 코스닥 종목명 및 코드 추출\n",
    "def extract_stocks_name():\n",
    "    df_kospi = pd.read_excel('kospi_code.xlsx', dtype={'단축코드': str})\n",
    "    df_kosdaq = pd.read_excel('kosdaq_code.xlsx', dtype={'단축코드': str})\n",
    "\n",
    "    filterd_df_kospi = df_kospi[df_kospi['그룹코드'] == 'ST']\n",
    "    filterd_df_kosdaq = df_kosdaq[df_kosdaq['증권그룹구분코드'] == 'ST']\n",
    "\n",
    "    result_df_kospi = filterd_df_kospi[['한글명', '단축코드']].copy()\n",
    "    result_df_kosdaq = filterd_df_kosdaq[['한글종목명', '단축코드']].copy()\n",
    "    \n",
    "    \n",
    "    result_df_kospi.rename(columns={'한글명': '종목명'}, inplace=True)\n",
    "    result_df_kosdaq.rename(columns={'한글종목명': '종목명'}, inplace=True)\n",
    "\n",
    "    result_df_kospi['단축코드'] = result_df_kospi['단축코드'].apply(lambda x: x.zfill(6))\n",
    "    result_df_kosdaq['단축코드'] = result_df_kosdaq['단축코드'].apply(lambda x: x.zfill(6))\n",
    "\n",
    "    result_df_kospi.to_csv('../data/kospi_stocks.csv', index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)\n",
    "    result_df_kosdaq.to_csv('../data/kosdaq_stocks.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"CSV 파일이 성공적으로 저장되었습니다: kospi_stocks.csv, kosdaq_stocks.csv\")\n",
    "\n",
    "    #df = pd.read_csv('kosdaq_stocks.csv')\n",
    "    #print(df['단축코드'].head(10))\n",
    "    \n",
    "extract_stocks_name()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_community_posts(base_url):\n",
    "    posts = []\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    today = datetime.now()\n",
    "    date_start = (today - timedelta(days=1)).replace(hour=0, minute=0, second=0)\n",
    "    date_end = (today - timedelta(days=1)).replace(hour=23, minute=59, second=59)\n",
    "    \n",
    "    page = 0\n",
    "    while True:\n",
    "        url = f\"{base_url}?&od=T31&category=0&po={page}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch page {page}: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = soup.select('div.list_item.symph_row')\n",
    "        \n",
    "        if not articles:\n",
    "            print(\"No more articles found.\")\n",
    "            break\n",
    "        \n",
    "        for article in articles:\n",
    "            # Extract title and link\n",
    "            title_element = article.select_one('span.subject_fixed')\n",
    "            date_element = article.select_one('span.timestamp')\n",
    "            link_element = article.select_one('a.list_subject')\n",
    "            \n",
    "            if not title_element or not date_element or not link_element:\n",
    "                continue\n",
    "            \n",
    "            title = title_element.get_text(strip=True)\n",
    "            date_str = date_element.get_text(strip=True)\n",
    "            link = f\"https://www.clien.net{link_element['href']}\"\n",
    "            \n",
    "            # Convert date to datetime object\n",
    "            post_date = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            # Stop if the post date is before range\n",
    "            if post_date < date_start:\n",
    "                print(\"No more posts from yesterday.\")\n",
    "                return posts\n",
    "\n",
    "            if not (date_start <= post_date <= date_end):\n",
    "                continue\n",
    "            \n",
    "            # Crawling post content\n",
    "            post_response = requests.get(link, headers=headers)\n",
    "            if post_response.status_code != 200:\n",
    "                print(f\"Failed to fetch post: {link}\")\n",
    "                continue\n",
    "            \n",
    "            post_soup = BeautifulSoup(post_response.text, 'html.parser')\n",
    "            content_element = post_soup.select_one('div.post_article')\n",
    "            content = content_element.get_text(strip=True) if content_element else \"No content\"\n",
    "            \n",
    "            posts.append({\n",
    "                'title': title,\n",
    "                'date': post_date.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "                'text': content,\n",
    "                'link' : link\n",
    "            })\n",
    "        page += 1\n",
    "\n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 39...\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n\tGetHandleVerifier [0x0041FD53+23747]\n\t(No symbol) [0x003A7D54]\n\t(No symbol) [0x0027BE53]\n\t(No symbol) [0x002BFCA6]\n\t(No symbol) [0x002BFEEB]\n\t(No symbol) [0x002FD852]\n\t(No symbol) [0x002E1E44]\n\t(No symbol) [0x002FB41E]\n\t(No symbol) [0x002E1B96]\n\t(No symbol) [0x002B3F3C]\n\t(No symbol) [0x002B4EBD]\n\tGetHandleVerifier [0x006FAC73+3017699]\n\tGetHandleVerifier [0x0070B93B+3086507]\n\tGetHandleVerifier [0x007040F2+3055714]\n\tGetHandleVerifier [0x004B5AF0+637536]\n\t(No symbol) [0x003B0A5D]\n\t(No symbol) [0x003ADA28]\n\t(No symbol) [0x003ADBC5]\n\t(No symbol) [0x003A07F0]\n\tBaseThreadInitThunk [0x75827BA9+25]\n\tRtlInitializeExceptionChain [0x7740C0CB+107]\n\tRtlClearBits [0x7740C04F+191]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-d172dd96dcd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;31m# 실행 예시\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[0mfm_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.fmkorea.com/index.php?mid=stock&category=2997203870\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m \u001b[0mfm_posts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_fmkorea_posts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfm_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[0mfm_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfm_posts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[0mfm_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/raw_data/fm_raw_data.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8-sig'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-d172dd96dcd7>\u001b[0m in \u001b[0;36mget_fmkorea_posts\u001b[1;34m(base_url)\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mWebDriverWait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpresence_of_element_located\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./../..\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py\u001b[0m in \u001b[0;36muntil\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0muntil_not\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n\tGetHandleVerifier [0x0041FD53+23747]\n\t(No symbol) [0x003A7D54]\n\t(No symbol) [0x0027BE53]\n\t(No symbol) [0x002BFCA6]\n\t(No symbol) [0x002BFEEB]\n\t(No symbol) [0x002FD852]\n\t(No symbol) [0x002E1E44]\n\t(No symbol) [0x002FB41E]\n\t(No symbol) [0x002E1B96]\n\t(No symbol) [0x002B3F3C]\n\t(No symbol) [0x002B4EBD]\n\tGetHandleVerifier [0x006FAC73+3017699]\n\tGetHandleVerifier [0x0070B93B+3086507]\n\tGetHandleVerifier [0x007040F2+3055714]\n\tGetHandleVerifier [0x004B5AF0+637536]\n\t(No symbol) [0x003B0A5D]\n\t(No symbol) [0x003ADA28]\n\t(No symbol) [0x003ADBC5]\n\t(No symbol) [0x003A07F0]\n\tBaseThreadInitThunk [0x75827BA9+25]\n\tRtlInitializeExceptionChain [0x7740C0CB+107]\n\tRtlClearBits [0x7740C04F+191]\n"
     ]
    }
   ],
   "source": [
    "def get_fmkorea_posts(base_url):\n",
    "    posts = []\n",
    "    \n",
    "    options = Options()\n",
    "    #options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    today = datetime.now()\n",
    "    yesterday = today - timedelta(days=1)\n",
    "    date_start = (today - timedelta(days=1)).replace(hour=0, minute=0, second=0)\n",
    "    date_end = (today - timedelta(days=1)).replace(hour=23, minute=59, second=59)\n",
    "    \n",
    "    page = 39\n",
    "    while True:\n",
    "        url = f\"{base_url}&page={page}\"\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(10)\n",
    "        \n",
    "        print(f\"Processing page {page}...\")\n",
    "        \n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, '.bd_lst.bd_tb_lst.bd_tb .title a')\n",
    "        stop_crawling = False\n",
    "        \n",
    "        if not articles:\n",
    "            print(\"No more articles found.\")\n",
    "            break\n",
    "        \n",
    "        for article in articles:\n",
    "        \n",
    "            if article.get_attribute('title') == \"댓글\":\n",
    "                continue\n",
    "            \n",
    "            title = article.text.strip()\n",
    "            url = article.get_attribute('href')\n",
    "            \n",
    "            parent_tr = article.find_element(By.XPATH, \"./../..\")  # article의 부모 tr 태그를 찾기\n",
    "            date_element = parent_tr.find_element(By.CSS_SELECTOR, \".time\")\n",
    "            date_str = date_element.text.strip()\n",
    "                \n",
    "            print(f\"Found article: {title}, URL: {url}, time: {date_str}\")\n",
    "                \n",
    "            try:\n",
    "                if \":\" in date_str: # HH:MM 형태 시간 계산\n",
    "                    post_time = datetime.strptime(date_str, \"%H:%M\").time()\n",
    "                    post_date = datetime.combine(today.date(), post_time)\n",
    "                    if post_date > today:  # HH:MM이 오늘 이후라면 어제로 조정\n",
    "                        post_date = datetime.combine(yesterday.date(), post_time)\n",
    "                else:\n",
    "                    post_date = datetime.strptime(date_str, \"%Y.%m.%d %H:%M\")\n",
    "            except Exception as e:\n",
    "                print(f\"Date parsing failed for {date_str}: {e}\")\n",
    "                continue\n",
    "                \n",
    "            if post_date < date_start:\n",
    "                stop_crawling = True\n",
    "                break\n",
    "            \n",
    "            if date_start <= post_date <= date_end:\n",
    "                # 게시글 본문 크롤링\n",
    "                print(f\"Crawling post: {title}\")\n",
    "                article.click()\n",
    "                time.sleep(2)\n",
    "                content_element = driver.find_element(By.CSS_SELECTOR, '.rd_body')\n",
    "                content = content_element.text.strip() if content_element else \"No content\"\n",
    "                \n",
    "                posts.append({\n",
    "                    'title': title,\n",
    "                    'date': post_date.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "                    'text': content,\n",
    "                    'link': url\n",
    "                })\n",
    "                \n",
    "                driver.back()\n",
    "                time.sleep(2)\n",
    "            \n",
    "            \n",
    "                \n",
    "        if stop_crawling:\n",
    "            print(\"Crawling stopped. All posts from yesterday have been fetched.\")\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "    \n",
    "    driver.quit()\n",
    "    return posts\n",
    "\n",
    "\n",
    "# 실행 예시\n",
    "fm_url = \"https://www.fmkorea.com/index.php?mid=stock&category=2997203870\"\n",
    "fm_posts = get_fmkorea_posts(fm_url) \n",
    "fm_df = pd.DataFrame(fm_posts)\n",
    "fm_df.to_csv('../data/raw_data/fm_raw_data.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopword.txt \n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = set(line.strip() for line in f if line.strip())\n",
    "    return stopwords\n",
    "\n",
    "# Load csv file \n",
    "def load_stock_names():\n",
    "    df_kospi = pd.read_csv('../data/raw_data/kospi_stocks.csv')  # KOSPI 종목명 파일\n",
    "    df_kosdaq = pd.read_csv('../data/raw_data/kosdaq_stocks.csv')  # KOSDAQ 종목명 파일\n",
    "    \n",
    "    # 종목명 컬럼 병합\n",
    "    stock_names = pd.concat([df_kospi['종목명'], df_kosdaq['종목명']])\n",
    "    stock_names = stock_names.dropna().str.strip().tolist()\n",
    "    stock_names.sort(key=len, reverse=True)\n",
    "    \n",
    "    return stock_names\n",
    "\n",
    "# Load alian json (대체어 저장 파일)\n",
    "def load_stock_aliases(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        alias_dict = json.load(f)\n",
    "    \n",
    "    return alias_dict\n",
    "\n",
    "# 대체어와 종목 명 match \n",
    "def match_stock_names_with_aliases(text, stock_names, stock_aliases):\n",
    "    matched_stocks = []\n",
    "    alias_to_main = {}  # Alias → 대표 종목명 매핑\n",
    "\n",
    "    # Alias 사전 구성\n",
    "    for main_stock, aliases in stock_aliases.items():\n",
    "        alias_to_main[main_stock] = main_stock  # 대표 종목명도 매핑\n",
    "        for alias in aliases:\n",
    "            alias_to_main[alias] = main_stock   # Alias → 대표 종목명 매핑\n",
    "\n",
    "    extended_stock_names = set(alias_to_main.keys())\n",
    "\n",
    "    # 긴 이름부터 매칭\n",
    "    for stock in sorted(extended_stock_names, key=len, reverse=True):\n",
    "        if stock in text:\n",
    "            matched_stocks.append(alias_to_main[stock])  # 대표 종목명으로 저장\n",
    "            text = text.replace(stock, '')               # 매칭된 종목명 제거\n",
    "\n",
    "    return matched_stocks, text\n",
    "\n",
    "# KoBERT를 사용해 키워드 추출\n",
    "def extract_keywords_with_kobert(text, tokenizer, model):\n",
    "    # 불용어 리스트 생성\n",
    "    STOPWORDS_FILE = '../data/stopwords.txt'\n",
    "    STOPWORDS = load_stopwords(STOPWORDS_FILE)\n",
    "    \n",
    "    keywords = []\n",
    "    \n",
    "    split_texts = [text[i:i+512] for i in range(0, len(text), 512)]\n",
    "    \n",
    "    for chunk in split_texts:\n",
    "        # KoBERT 토큰화\n",
    "        inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # 토큰 ID 배열 -> 토큰으로 변환\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        \n",
    "        # 토큰 재결합 (서브워드 합치기)\n",
    "        combined_tokens = []\n",
    "        temp_token = ''\n",
    "        for token in tokens:\n",
    "            if token.startswith('##'):\n",
    "                temp_token += token[2:]  # ## 제거 후 이어붙이기\n",
    "            else:\n",
    "                if temp_token:\n",
    "                    combined_tokens.append(temp_token)\n",
    "                    temp_token = ''\n",
    "                if len(token) > 1 and re.match(r'[가-힣a-zA-Z]+', token):\n",
    "                    combined_tokens.append(token)\n",
    "        \n",
    "        # 마지막 남은 토큰 추가\n",
    "        if temp_token:\n",
    "            combined_tokens.append(temp_token)\n",
    "        \n",
    "        # STOPWORDS 필터링\n",
    "        keywords.extend([token for token in combined_tokens if token not in STOPWORDS])\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def extract_stock_keywords(posts):\n",
    "    stock_names = load_stock_names()\n",
    "    stock_aliases = load_stock_aliases('../data/raw_data/stock_alias.json')\n",
    "    \n",
    "    # Load KoBERT model, tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "    model = BertModel.from_pretrained('monologg/kobert')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for post in posts:\n",
    "        text = post['title'] + \" \" + post['text']\n",
    "        text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', text) # 특수 문자 제거 및 전처리\n",
    "        \n",
    "        # 1. 종목명 및 Alias 매칭\n",
    "        matched_stocks, text = match_stock_names_with_aliases(text, stock_names, stock_aliases)\n",
    "        # 2. KoBERT 키워드 추출 및 재결합\n",
    "        kobert_keywords = extract_keywords_with_kobert(text, tokenizer, model)\n",
    "        \n",
    "        keyword_counts = Counter(matched_stocks + kobert_keywords)\n",
    "        keyword_counts_dict = dict(sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)) # 내림차순 정렬\n",
    "        \n",
    "        results.append({\n",
    "            'title': post['title'],\n",
    "            'date': post['date'],\n",
    "            'text': post['text'],\n",
    "            'link': post['link'],\n",
    "            'keywords': keyword_counts_dict\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more posts from yesterday.\n",
      "Failed to fetch page 1: 430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    clien_url = \"https://www.clien.net/service/board/cm_stock\"\n",
    "    fm_url = \"https://www.fmkorea.com/index.php?mid=stock&category=2997203870\"\n",
    "    clien_posts = get_clien_posts(clien_url)\n",
    "    fm_posts = get_fmkorea_posts(fm_url)\n",
    "    \n",
    "    clien_raw_data_df = extract_stock_keywords(clien_posts)\n",
    "    fm_raw_data_df = extract_stock_keywords(fm_posts)\n",
    "\n",
    "    combined_df = pd.concat([clien_raw_data_df, fm_raw_data_df], ignore_index=True)\n",
    "    combined_df.to_csv('../data/raw_data/community_raw_data.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
